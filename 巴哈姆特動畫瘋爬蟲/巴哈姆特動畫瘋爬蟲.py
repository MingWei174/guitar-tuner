import requests
from bs4 import BeautifulSoup
import pandas as pd
import re # æ–°å¢ï¼šç”¨ä¾†æŠ“å–å¹´ä»½æ•¸å­—çš„æ­£å‰‡è¡¨é”å¼
import time
import random
from datetime import datetime, timedelta

# --- æ–°å¢ï¼šå¼•å…¥ Rich æ¨¡çµ„ ---
from rich.console import Console
from rich.table import Table
from rich import box  # ç”¨ä¾†è¨­å®šè¡¨æ ¼é‚Šæ¡†æ¨£å¼

# åˆå§‹åŒ– Rich çš„æ§åˆ¶å°
console = Console()


def get_status_by_date(soup, year):
    """
    æ ¸å¿ƒé‚è¼¯ï¼šå¾å…§é æ‰¾å‡ºæœ€æ–°ä¸€é›†çš„æ—¥æœŸï¼Œåˆ¤æ–·æ˜¯å¦å®Œçµ
    """
    try:
        # 1. æŠ“å–æ‰€æœ‰é›†æ•¸çš„å€å¡Š (é€šå¸¸åœ¨ section.season è£¡é¢)
        # å·´å“ˆçš„çµæ§‹é€šå¸¸æ˜¯ <div class="season"> ... <a>...<span class="date">12/08</span></a>
        # æˆ‘å€‘ç›´æ¥ç”¨ regex åœ¨æ•´å€‹ç¶²é æ–‡å­—ä¸­æ‰¾ "MM/DD" é€™ç¨®æ ¼å¼çš„æ—¥æœŸ
        # é€™ç¨®æš´åŠ›æ³•æœ€é€šç”¨ï¼Œä¸ç”¨æ€• class æ”¹å
        
        text_content = soup.get_text()
        
        # å°‹æ‰¾æ‰€æœ‰åƒ "12/08" æˆ– "01/05" é€™æ¨£çš„æ—¥æœŸ
        date_matches = re.findall(r'(\d{1,2})/(\d{1,2})', text_content)
        
        if not date_matches:
            return "é€£è¼‰ä¸­" # æŠ“ä¸åˆ°æ—¥æœŸï¼Œä¿å®ˆèµ·è¦‹ç•¶ä½œé€£è¼‰ä¸­

        # 2. è½‰æ›æ—¥æœŸä¸¦æ‰¾å‡ºã€Œé›¢ç¾åœ¨æœ€è¿‘ã€çš„ä¸€å€‹æ—¥æœŸ
        latest_date = None
        today = datetime.now()
        
        for match in date_matches:
            month, day = int(match[0]), int(match[1])
            
            # ç°¡å–®é˜²å‘†ï¼šæœˆä»½ä¸èƒ½è¶…é 12
            if month > 12 or month < 1: continue

            # çµ„åˆå¹´ä»½ï¼šå¦‚æœæ˜¯ 12æœˆï¼Œè€Œç¾åœ¨æ˜¯ 1æœˆï¼Œé‚£å¯èƒ½æ˜¯å»å¹´çš„ 12æœˆ
            # ä½†ç‚ºäº†ç°¡åŒ–ï¼Œæˆ‘å€‘å…ˆå‡è¨­éƒ½æ˜¯ç•¶å¹´åº¦ (2025) çš„æ—¥æœŸ
            # é™¤éè©²ç•ªæ˜¯è·¨å¹´ä»½çš„ (é€™éƒ¨åˆ†é‚è¼¯å¯ä»¥å¯«æ›´ç´°ï¼Œä½†æœŸæœ«å°ˆé¡Œå…ˆä¸ç”¨å¤ªè¤‡é›œ)
            try:
                date_obj = datetime(year, month, day)
                
                # æ›´æ–° logic: æ‰¾å‡ºæœ€æ¥è¿‘ä»Šå¤©ï¼Œä½†ä¸æ˜¯æœªä¾†çš„æ—¥æœŸ (æœ‰äº›é å‘Šæœƒæœ‰æœªä¾†æ—¥æœŸ)
                if date_obj <= today:
                    if latest_date is None or date_obj > latest_date:
                        latest_date = date_obj
            except:
                continue
        
        if latest_date is None:
            return "é€£è¼‰ä¸­"

        # 3. ã€é—œéµåˆ¤æ–·ã€‘ å…©é€±æ³•å‰‡
        days_diff = (today - latest_date).days
        print(f"    -> æœ€æ–°ä¸€é›†æ—¥æœŸ: {latest_date.strftime('%m/%d')}, è·é›¢ä»Šå¤© {days_diff} å¤©")
        
        if days_diff > 14:
            return "å·²å®Œçµ"
        else:
            return "é€£è¼‰ä¸­"

    except Exception as e:
        print(f"    æ—¥æœŸåˆ¤æ–·éŒ¯èª¤: {e}")
        return "é€£è¼‰ä¸­" # ç™¼ç”ŸéŒ¯èª¤æ™‚çš„é è¨­å€¼
    

def get_anime_details(link, year):
    """
    çˆ¬å–å·´å“ˆå§†ç‰¹å‹•ç•«ç˜‹çš„åˆ—è¡¨è³‡æ–™
    """
    url = "https://ani.gamer.com.tw/animeList.php?sort=2" # sort=1 ä»£è¡¨ä¾å¹´ä»½æ’åº # sort=2 ä»£è¡¨ä¾æœˆäººæ°£æ’åº(æˆ‘è¦çš„,æ‰èƒ½æŠ“åˆ°ä»¥å‰çš„ç¥ä½œ)
    
    # 2. æŠ€è¡“é‡é»ï¼šUser-Agent å½è£ (å°‡åœ¨å ±å‘Šä¸­æåŠ) # ä¿®æ”¹ Headersï¼šåŠ å…¥ Cookie å½è£æˆå·²æ»¿ 18 æ­²çš„ä½¿ç”¨è€…
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
        "Cookie": "over18=1"  # <--- é—œéµï¼åŠ å…¥é€™ä¸€è¡Œ
    }
    
    try:
        # éš¨æ©Ÿä¼‘æ¯ 0.5 ~ 1.5 ç§’ï¼Œæ¨¡æ“¬äººé¡é»æ“Šï¼Œé¿å…è¢«é–
        time.sleep(random.uniform(0.5, 1.5)) 
        res = requests.get(link, headers=headers, timeout=10)
        if res.status_code != 200:
            return 0.0
        
        soup = BeautifulSoup(res.text, "html.parser")
        
        # 1. æŠ“è©•åˆ†
        score_div = soup.find("div", class_="score-overall-number")
        score = float(score_div.text.strip()) if score_div else 0.0
        
        # 2. æŠ“ç‹€æ…‹ (ä½¿ç”¨ä¸Šé¢çš„æ™‚é–“åˆ¤æ–·é‚è¼¯)
        # å¦‚æœæ˜¯èˆŠç•ª(2024ä»¥å‰)ï¼Œç›´æ¥å›å‚³å·²å®Œçµï¼Œä¸ç”¨æµªè²»æ™‚é–“ç®—æ—¥æœŸ
        current_year = datetime.now().year
        if isinstance(year, int) and year < current_year:
            real_status = "å·²å®Œçµ"
        else:
            # å¦‚æœæ˜¯ä»Šå¹´(2025)çš„ï¼Œæ‰å»åˆ†ææ—¥æœŸ
            real_status = get_status_by_date(soup, year if isinstance(year, int) else current_year)
        
        # --- 3. æŠ“å–æ¨™ç±¤ (çµ‚æ¥µä¿®æ­£) ---
        tags = []
        
        # ç­–ç•¥ A: ä½¿ç”¨ CSS Selector ç›´æ¥æ‰¾ class="tag" çš„ li
        # é€™æ˜¯æœ€é€šç”¨çš„æ–¹æ³•ï¼Œä¸ç®¡ ul å«ä»€éº¼åå­—éƒ½èƒ½æŠ“åˆ°
        tag_elements = soup.select("li.tag")
        
        if tag_elements:
            tags = [t.text.strip() for t in tag_elements]
        else:
            # ç­–ç•¥ B: å¦‚æœç­–ç•¥ A å¤±æ•—ï¼Œå˜—è©¦æŠ“å– "ä½œå“åˆ†é¡" é™„è¿‘çš„æ–‡å­— (å‚™ç”¨æ–¹æ¡ˆ)
            # æœ‰æ™‚å€™ requests æŠ“åˆ°çš„ HTML çµæ§‹æ¯”è¼ƒäº‚ï¼Œç”¨æ–‡å­—å®šä½
            try:
                data_intro = soup.find("div", class_="data_intro")
                if data_intro:
                    # é€™è£¡é¢é€šå¸¸åŒ…å«æ¨™ç±¤ï¼Œè©¦è‘—æ‰¾è£¡é¢çš„é€£çµæ–‡å­—
                    links = data_intro.find_all("a")
                    # éæ¿¾æ‰éæ¨™ç±¤çš„é€£çµ (é€šå¸¸æ¨™ç±¤é€£çµåŒ…å« search.php?keyword=)
                    tags = [a.text.strip() for a in links if "keyword=" in a.get("href", "")]
            except:
                pass

        # Debug: çœŸçš„æŠ“ä¸åˆ°æ‰å°
        if not tags:
            print(f"   [è­¦å‘Š] {link} çœŸçš„æŠ“ä¸åˆ°æ¨™ç±¤ï¼")

        tags_str = ",".join(tags)

        return score, real_status, tags_str
            
    except Exception as e:
        print(f"å…§é éŒ¯èª¤: {e}")
        return 0.0, "é€£è¼‰ä¸­"

def get_anime_data_v3(max_pages=11):
    """
    å‡ç´šç‰ˆï¼šæ”¯æ´ç¿»é  + å…§é çˆ¬å–
    max_pages: æƒ³è¦çˆ¬å¹¾é  (å»ºè­°å…ˆè¨­ 5 é æ¸¬è©¦ï¼Œæ­£å¼å ±å‘Šå¯ä»¥è¨­ 10 æˆ– 20)
    """
    base_url = "https://ani.gamer.com.tw/animeList.php?sort=2"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36"
    }

    all_data = []

    for page in range(1, max_pages + 1):
        # ä½¿ç”¨ console.print å¯ä»¥å°å‡ºæœ‰é¡è‰²çš„å­—
        console.print(f"[bold cyan]--- æ­£åœ¨çˆ¬å–ç¬¬ {page} é  ---[/bold cyan]")
        # sort=2 ä»£è¡¨ä¾äººæ°£æ’åº (ç´¯ç©è§€çœ‹æ•¸)ï¼Œé€™æ¨£æ¯”è¼ƒå®¹æ˜“æŠ“åˆ°èˆŠçš„ç¥ä½œ
        url = f"{base_url}?sort=1&page={page}"
        
        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            print(f"ç¬¬ {page} é é€£ç·šå¤±æ•—")
            continue

        soup = BeautifulSoup(response.text, "html.parser")
        anime_items = soup.find_all("a", class_="theme-list-main") 

        print(f"  > æœ¬é æ‰¾åˆ° {len(anime_items)} éƒ¨å‹•ç•«ï¼Œé–‹å§‹é€²å…¥å…§é æŠ“è©•åˆ†...")

        for item in anime_items:
            title = item.find("p", class_="theme-name").text.strip()
            view_count_str = item.find("div", class_="show-view-number").find("p").text.strip()
            info_text = item.find("p", class_="theme-time").text.strip()
            
            # å–å¾—å…§é é€£çµ (href)
            href = item.get('href')
            full_link = f"https://ani.gamer.com.tw/{href}"
            
            # 1. è™•ç†è§€çœ‹æ•¸
            if "è¬" in view_count_str:
                view_count = int(float(view_count_str.replace("è¬", "")) * 10000)
            elif view_count_str.isdigit():
                view_count = int(view_count_str)
            else:
                view_count = 0
                
            # è™•ç†å¹´ä»½ (ä¿®æ­£å¾Œ)
            # åŸå§‹å¯«æ³•: re.search(r'^\d{4}', info_text) -> éŒ¯èª¤ï¼Œå› ç‚ºé–‹é ­æ˜¯ä¸­æ–‡
            year_match = re.search(r'\d{4}', info_text)  # âœ… ä¿®æ­£ï¼šæ‹¿æ‰ ^
            
            if year_match:
                year = int(year_match.group())
            else:
                # ç‚ºäº†é™¤éŒ¯ï¼Œå»ºè­°é€™è£¡å¯ä»¥æŠŠæŠ“ä¸åˆ°çš„å­—å°å‡ºä¾†çœ‹çœ‹é•·æ€æ¨£
                print(f" [Debug] æŠ“ä¸åˆ°å¹´ä»½ï¼ŒåŸå§‹æ–‡å­—æ˜¯: {info_text}") 
                year = "æœªçŸ¥"

            # è™•ç†é¡Œæ
            is_isekai = "æ˜¯" if ('ç•°ä¸–ç•Œ' in title or 'è½‰ç”Ÿ' in title) else "å¦"

            # ã€é€²å…¥å…§é ã€‘åŒæ™‚æŠ“è©•åˆ† + åˆ¤æ–·ç‹€æ…‹
            # ç°¡åŒ–è¼¸å‡ºï¼Œè®“ç•«é¢ä¹¾æ·¨ä¸€é»
            console.print(f"  > åˆ†æ: [yellow]{title}[/yellow] ({year})...", end="\r")
            
            # --- ä¿®æ”¹ï¼šæ¥æ”¶ä¸‰å€‹å›å‚³å€¼ (å¤šäº† tags_str) ---
            real_score, real_status, tags_str = get_anime_details(full_link, year)
            
            # é¡¯ç¤ºçµæœçµ¦ä½ çœ‹
            # print(f"      -> åˆ¤å®šç‚º: {real_status}, è©•åˆ†: {real_score}")

            all_data.append({
                "å‹•ç•«åç¨±": title,
                "è§€çœ‹æ¬¡æ•¸": view_count,
                "å¹´ä»½": year,
                "ç‹€æ…‹": real_status, # ä½¿ç”¨æ–°çš„æ™‚é–“åˆ¤æ–·çµæœ
                "æ˜¯å¦ç•°ä¸–ç•Œ": is_isekai,
                "è©•åˆ†": real_score, # é€™æ˜¯çœŸå¯¦çš„äº†ï¼
                "ä¸»é¡Œæ¨™ç±¤": tags_str  # æ–°å¢é€™ä¸€æ¬„
            })
            
        # æ¯ä¸€é çˆ¬å®Œä¼‘æ¯ä¸€ä¸‹
        time.sleep(2)
    
    return all_data

# --- ã€å®‰å…¨æ¨¡å¼ã€‘ä¿è­‰é¡¯ç¤ºç‰ˆ ---
def print_rich_table(df):
    """
    ä½¿ç”¨ Rich æ¨¡çµ„ç¹ªè£½è¡¨æ ¼ (å¼·åˆ¶å…¨é’è‰²/ç™½è‰²é…è‰²ï¼Œé¿å…é»‘åº•é»‘å­—å•é¡Œ)
    """
    # 1. æ¨™é¡Œèˆ‡é‚Šæ¡†ï¼šå¼·åˆ¶æ¨™é¡Œç‚ºé’è‰²
    table = Table(
        title="[cyan]ğŸ“Š å·´å“ˆå§†ç‰¹å‹•ç•«ç˜‹ - çˆ¬èŸ²åˆ†æå ±å‘Š[/cyan]", 
        box=box.ROUNDED, 
        header_style="bold cyan",  # è¡¨é ­å¼·åˆ¶é’è‰²
        show_lines=True            # é¡¯ç¤ºåˆ†éš”ç·šï¼Œçœ‹å¾—æ›´æ¸…æ¥š
    )

    # 2. è¨­å®šæ¬„ä½ (å…¨éƒ¨é å·¦æˆ–ç½®ä¸­ï¼Œä¸è¨­ style ä»¥å…è®Šé»‘)
    table.add_column("æ’å", justify="center")
    table.add_column("å‹•ç•«åç¨±", justify="left", no_wrap=False, max_width=30, overflow="fold")
    table.add_column("å¹´ä»½", justify="center")
    table.add_column("ç‹€æ…‹", justify="center")
    table.add_column("è§€çœ‹æ•¸", justify="right")
    table.add_column("è©•åˆ†", justify="right")
    # --- æ–°å¢é€™æ¬„ ---
    # max_width=20 åŠ ä¸Š overflow="fold" ä»£è¡¨å¦‚æœæ¨™ç±¤å¤ªå¤šï¼Œæœƒè‡ªå‹•æ›è¡Œé¡¯ç¤ºï¼Œä¸æœƒåˆ‡æ‰
    table.add_column("ä¸»é¡Œæ¨™ç±¤", justify="left", style="magenta", max_width=20, overflow="fold")
    table.add_column("ç•°ä¸–ç•Œ", justify="center")

    # 3. é€è¡ŒåŠ å…¥è³‡æ–™ (å…¨éƒ¨å¼·åˆ¶åŠ ä¸Šé¡è‰²æ¨™ç±¤)
    for index, row in df.iterrows():
        
        # ç‚ºäº†ä¿éšªï¼Œæ‰€æœ‰æ¬„ä½éƒ½åŠ ä¸Š [white] æˆ– [cyan] æ¨™ç±¤
        # å¦‚æœä½ çš„èƒŒæ™¯æ˜¯é»‘çš„ï¼Œwhite ä¸€å®šçœ‹å¾—åˆ°
        
        rank_str = f"[white]{index + 1}[/]"
        name_str = f"[cyan]{row['å‹•ç•«åç¨±']}[/]"  # ä½ åŸæœ¬çœ‹å¾—åˆ°é€™å€‹ï¼Œæ‰€ä»¥ç¹¼çºŒç”¨é’è‰²
        year_str = f"[white]{row['å¹´ä»½']}[/]"
        
        # ç‹€æ…‹
        if row['ç‹€æ…‹'] == "å·²å®Œçµ":
            status_str = f"[bold red]{row['ç‹€æ…‹']}[/]" # ç´…è‰²é€šå¸¸åœ¨é»‘åº•ä¹Ÿå¾ˆæ¸…æ¥š
        else:
            status_str = f"[bold green]{row['ç‹€æ…‹']}[/]"
        
        # è§€çœ‹æ•¸ (å¼·åˆ¶ç™½è‰²)
        view_str = f"[white]{row['è§€çœ‹æ¬¡æ•¸']/10000:.1f}è¬[/]"

        # è©•åˆ† (å¼·åˆ¶ç™½è‰²ï¼Œé«˜åˆ†ç”¨é»ƒè‰²)
        if row['è©•åˆ†'] >= 9.5:
            score_str = f"[bold yellow]{row['è©•åˆ†']}[/]"
        else:
            score_str = f"[white]{row['è©•åˆ†']}[/]"

        # è™•ç†ä¸»é¡Œæ¨™ç±¤ 
        # æª¢æŸ¥æ˜¯å¦ç‚ºç©ºå€¼ (NaN)ï¼Œå¦‚æœæ˜¯å°±é¡¯ç¤º "-"
        raw_tags = row.get('ä¸»é¡Œæ¨™ç±¤', '')
        if pd.isna(raw_tags) or raw_tags == "":
            tags_str = "-"
        else:
            # å°‡é€—è™Ÿæ›æˆç©ºæ ¼ï¼Œè¦–è¦ºä¸Šæ¯”è¼ƒä¹¾æ·¨ï¼Œæˆ–è€…ä¿ç•™é€—è™Ÿä¹Ÿå¯ä»¥
            # é€™è£¡è¨­å®šç‚ºç´«è‰² (magenta)
            tags_str = f"[magenta]{raw_tags}[/]"
        
        # ç•°ä¸–ç•Œ (æ”¹æˆæ–‡å­— YES/NO é¿å…äº‚ç¢¼)
        isekai_str = f"[white]{'YES' if row['æ˜¯å¦ç•°ä¸–ç•Œ'] == 'æ˜¯' else '-'}[/]"

        # åŠ å…¥ Row (è¨˜å¾—é †åºè¦è·Ÿä¸Šé¢çš„ add_column ä¸€æ¨£)
        table.add_row(
            rank_str,
            name_str,
            year_str,
            status_str,
            view_str,
            score_str,
            tags_str, # æ–°å¢çš„è®Šæ•¸æ”¾é€™è£¡
            isekai_str
        )

    console.print(table)

# --- åŸ·è¡Œçˆ¬èŸ²ä¸¦å­˜æª” ---
if __name__ == "__main__":
    
    console.print("[bold green]ğŸš€ çˆ¬èŸ²å•Ÿå‹•ä¸­...[/bold green]")

    # è¨­å®šè¦çˆ¬å¹¾é ï¼Ÿå»ºè­°å…ˆè¨­ 5 é è©¦è·‘ï¼Œç¢ºèªæ²’å•é¡Œå¾Œå†æ”¹æˆ 10 æˆ– 20 é æŠ“æ›´å¤šè³‡æ–™
    # 5 é å¤§ç´„éœ€è¦ 2-3 åˆ†é˜ (å› ç‚ºè¦é€²å…§é )
    data = get_anime_data_v3(max_pages=11) 
    
    df = pd.DataFrame(data)

    # 1. å­˜æª”
    output_file = "anime_data.xlsx"
    df.to_excel(output_file, index=False)
    
    print("\n" + "="*50)
    
    # 2. å†å‘¼å« Rich è¡¨æ ¼
    if not df.empty:
        df_sorted = df.sort_values(by="è§€çœ‹æ¬¡æ•¸", ascending=False).reset_index(drop=True)
        print_rich_table(df_sorted)
    else:
        console.print("[bold red]âŒ æ²’æœ‰æŠ“åˆ°ä»»ä½•è³‡æ–™ï¼[/bold red]")